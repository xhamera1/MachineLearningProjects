# -*- coding: utf-8 -*-
"""lab04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17qdt3UnZHMeDGcL4Fi7wYluAGDmg8E39

# BREAST CANCER DATA
"""

# ZADANIE 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets

data_breast_cancer = datasets.load_breast_cancer(as_frame=True)

print(data_breast_cancer.frame[['mean area', 'mean smoothness', 'target']])

print(data_breast_cancer['DESCR'])
print(data_breast_cancer['feature_names'])
print(data_breast_cancer.target_names)
# 'malignant' - złośliwy
# 'benign'    - łagodny

from sklearn.model_selection import train_test_split

X_bc = pd.DataFrame(data_breast_cancer.data, columns=data_breast_cancer.feature_names)
y_bc = pd.Series(data_breast_cancer.target)

# biore pod uwage jedynie cechy: 'mean area' i 'mean smoothness'
X_bc = X_bc[['mean area', 'mean smoothness']]

# print(X_bc.dtypes)
# print(y_bc.dtypes)

X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(X_bc, y_bc, test_size=0.2, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

# bez skalowania
svm_clf_bc = LinearSVC(C=1.0, loss='hinge', random_state=42)
svm_clf_bc.fit(X_bc_train, y_bc_train)


# ze skalowaniem
svm_clf_bc_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svc", LinearSVC(C=1.0, loss="hinge", random_state=42)),
])
svm_clf_bc_scaled.fit(X_bc_train, y_bc_train)

from sklearn.metrics import accuracy_score

train_bc_acc = accuracy_score(y_bc_train, svm_clf_bc.predict(X_bc_train))
test_bc_acc = accuracy_score(y_bc_test, svm_clf_bc.predict(X_bc_test))

train_bc_scaled_acc = accuracy_score(y_bc_train, svm_clf_bc_scaled.predict(X_bc_train))
test_bc_scaled_acc = accuracy_score(y_bc_test, svm_clf_bc_scaled.predict(X_bc_test))

bc_acc_list = [train_bc_acc, test_bc_acc, train_bc_scaled_acc, test_bc_scaled_acc]
print(bc_acc_list)
# [0.6285714285714286, 0.6228070175438597, 0.8923076923076924, 0.9298245614035088]
# skalowanie dalo zdecydowanie lepsze resultaty accuracy

import pickle

with open('bc_acc.pkl', 'wb') as file:
  pickle.dump(bc_acc_list, file)

"""# IRIS DATA"""

data_iris = datasets.load_iris(as_frame=True)
print(data_iris['DESCR'])
print(data_iris['target_names'])

# eksperyment dla zbioru irysow wykrywajacy czy dany przypadek jest gatunku Virginica
# na podstawie cech: dlugosc i szerokosc platka
# 0: sepal length
# 1: sepal width
# 2: petal length
# 3: petal widt

X_iris = data_iris['data'].iloc[:, [2,3]] # dlugosc i szerokosc platka, iloc bo ja juz mam to jako dataframe
y_iris = (data_iris['target'] == 2).astype(np.int8) # 2 to iris virginica

# print(X_iris.loc[[0,1,2]])
# print(y_iris.loc[[0,1,2]])

X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)

# bez skalowania
svm_iris_clf = LinearSVC(C=1.0, loss='hinge', random_state=42)
svm_iris_clf.fit(X_iris_train, y_iris_train)

# z skalowaniem
svm_iris_clf_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svc", LinearSVC(C=1.0, loss='hinge', random_state=42)),
])

svm_iris_clf_scaled.fit(X_iris_train, y_iris_train)

train_iris_acc = accuracy_score(y_iris_train, svm_iris_clf.predict(X_iris_train))
test_iris_acc = accuracy_score(y_iris_test, svm_iris_clf.predict(X_iris_test))
train_iris_acc_scaled = accuracy_score(y_iris_train, svm_iris_clf_scaled.predict(X_iris_train))
test_iris_acc_scaled = accuracy_score(y_iris_test, svm_iris_clf_scaled.predict(X_iris_test))

iris_acc_list = [train_iris_acc, test_iris_acc, train_iris_acc_scaled, test_iris_acc_scaled]
print(iris_acc_list)
# [0.9416666666666667, 1.0, 0.9416666666666667, 1.0]
# zarowno bez jak i z skalowaniem mamy zadziwiajaco, podejrzanie dobre accuracy (takie samo bez i z skalowaniem),
# wynika to pewnie z faktu, ze zbior danych jest maly (150 probek)

with open('iris_acc.pkl', 'wb') as file:
  pickle.dump(iris_acc_list, file)

"""# Regresja liniowa SVM

"""

# ZADANIE 4
size = 900
X = np.random.rand(size)*5 - 2.5
w4, w3, w2, w1, w0 = 1, 2 , 1, -4, 2
y = w4*(X**4) + w3*(X**3) + w2*(X**2) + w1*X + w0 + np.random.randn(size)*8-4
df = pd.DataFrame({'x': X, 'y': y})
df.plot.scatter(x='x',y='y')

X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1,1), y, test_size=0.2, random_state=42)
# print(X_train)
# print(y_train)

from sklearn.svm import LinearSVR
from sklearn.preprocessing import PolynomialFeatures

svm_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=4, include_bias=False)),
    ('linear_svr', LinearSVR()),
])

svm_reg.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error

mse_train = mean_squared_error(y_train, svm_reg.predict(X_train))
mse_test = mean_squared_error(y_test, svm_reg.predict(X_test))

print(mse_train, mse_test)
# 60.67299812148146 67.54348540812032
# rzeczywiscie podobne wyniki do do algorytmow KNN oraz LinearReggresion dla wielomianow

plt.scatter(X, y, color='blue')
plt.scatter(X_test, svm_reg.predict(X_test), color="yellow", marker='v')
plt.scatter(X_train, svm_reg.predict(X_train), color="red", marker='^')
plt.xlim(-3,3)
plt.ylim(-30, 100)
plt.show()

# SVR z kernelem poly 4 stopnia

from sklearn.svm import SVR

svm_poly_reg = SVR(kernel='poly', degree=4)
svm_poly_reg.fit(X_train, y_train)

mse_train_poly = mean_squared_error(y_train, svm_poly_reg.predict(X_train))
mse_test_poly = mean_squared_error(y_test, svm_poly_reg.predict(X_test))

print(mse_train_poly, mse_test_poly)
# 105.25837856194626 117.39833119803865
# zdecydowanie wieksze (gorsze) wyniki mse co moze byc wynikiem zle dobranych parametrow

# szukanie najlepszych parametrow coef0 i C

from sklearn.model_selection import GridSearchCV

param_grid = {
    'C' : [0.1, 1, 10],       # C - parametr regularyzacji, wysokie = model przykleja sie do danych, niskie = model bardziej ogolny
    'coef0' : [0.1, 1, 10]    # coef0 - parametr stosowany w funckjach jadrowych (dodaje stala wartosc do obliczen)
}

svr_poly = SVR(kernel='poly', degree=4)

search = GridSearchCV(svr_poly,
                      param_grid,
                      scoring='neg_mean_squared_error',  # w GridSearchCV ocena jest maksymalizowana wiec tu musi byc ujemna
                      n_jobs=-1)
search.fit(X.reshape(-1,1), y)

print(f'Best negative mean squared error = {search.best_score_}')
print(search.best_params_)
# Best negative mean squared error = -62.59189445816537
# {'C': 10, 'coef0': 1}

# SVR dla wyliczonych optymalnych parametrow

svr_poly_opt = SVR(kernel='poly', degree=4, C=10, coef0=1)
svr_poly_opt.fit(X_train, y_train)

train_mse_opt = mean_squared_error(y_train, svr_poly_opt.predict(X_train))
test_mse_opt = mean_squared_error(y_test, svr_poly_opt.predict(X_test))

print(train_mse_opt, test_mse_opt)
# 60.55587168191989 67.42649268003711
# zdecydowanie lepsze

plt.scatter(X, y, color='blue')
plt.scatter(X_test, svr_poly_opt.predict(X_test), color="yellow", marker='v')
plt.scatter(X_train, svr_poly_opt.predict(X_train), color="red", marker='^')
plt.xlim(-3,3)
plt.ylim(-30, 100)
plt.show()

reg_mse_list = [mse_train, mse_test, train_mse_opt, test_mse_opt]
print(reg_mse_list)
# [60.67299812148146, 67.54348540812032, 60.55587168191989, 67.42649268003711]

with open('reg_mse.pkl', 'wb') as file:
  pickle.dump(reg_mse_list, file)